{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_avVIGTpvioI"
      },
      "source": [
        "# DX 704 Week 3 Project\n",
        "\n",
        "This week's project will give you practice with optimizing choices for bandit algorithms.\n",
        "You will be given access to the bandit problem via a blackbox object, and you will investigate the bandit rewards to pick a suitable algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftRkegOQowWA"
      },
      "source": [
        "The full project description, a template notebook and supporting code are available on GitHub: [Project 3 Materials](https://github.com/bu-cds-dx704/dx704-project-03).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tPHvNSEdR6h"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.4.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VKDAEY8JMI"
      },
      "source": [
        "## Part 1: Pick a Bandit Algorithm\n",
        "\n",
        "Experiment with the multi-armed bandit interface using seed 0 to learn about the distribution of rewards and decide what kind of bandit algorithm will be appropriate.\n",
        "A histogram will likely be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BCggNE7NpiQN"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class BanditProblem(object):\n",
        "    def __init__(self, seed):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.num_arms = 3\n",
        "        self.ns = self.rng.integers(low=1, high=10, size=self.num_arms)\n",
        "        self.ps = self.rng.uniform(low=0.2, high=0.4, size=self.num_arms)\n",
        "\n",
        "    def get_num_arms(self):\n",
        "        return self.num_arms\n",
        "\n",
        "    def get_reward(self, arm):\n",
        "        if arm < 0 or arm >= self.num_arms:\n",
        "            raise ValueError(\"Invalid arm\")\n",
        "\n",
        "        x = self.rng.uniform()\n",
        "        x *= self.rng.binomial(self.ns[arm], self.ps[arm])\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "X99ZQUyhpgak"
      },
      "outputs": [],
      "source": [
        "bandit0 = BanditProblem(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "frDtVjt4qATJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_num_arms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sdM9Ec3HqC6h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.8255111545554434"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_reward(arm=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iuQ0jCr_plcZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arm 0: mean=0.815, std=0.784\n",
            "Arm 1: mean=0.572, std=0.634\n",
            "Arm 2: mean=0.927, std=0.848\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Explore reward distributions for each arm\n",
        "num_samples = 1000\n",
        "rewards = [[] for _ in range(bandit0.get_num_arms())]\n",
        "\n",
        "for arm in range(bandit0.get_num_arms()):\n",
        "    for _ in range(num_samples):\n",
        "        rewards[arm].append(bandit0.get_reward(arm))\n",
        "\n",
        "# Print basic statistics\n",
        "for arm in range(bandit0.get_num_arms()):\n",
        "    rewards_arm = np.array(rewards[arm])\n",
        "    print(f\"Arm {arm}: mean={rewards_arm.mean():.3f}, std={rewards_arm.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-3KtNXtzlY"
      },
      "source": [
        "Based on your investigation, pick an appropriate bandit algorithm to implement from the algorithms covered this week.\n",
        "Write a file \"algorithm-choice.txt\" that states your choice and give a single sentence justifying your choice and rejecting the alternatives.\n",
        "Keep your explanation concise; you should be able to justify your choice solely based on the type of numbers observed, and whether those match the bandit algorithms that you have learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I chose the Upper Confidence Bound (UCB) algorithm because the rewards are continuous, bounded, and stochastic, which matches UCB’s assumptions, while ε-greedy ignores uncertainty structure and Thompson Sampling assumes specific reward distributions (e.g., Bernoulli or Gaussian) that do not apply here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"algorithm-choice.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"I chose the Upper Confidence Bound (UCB) algorithm because the rewards are continuous and stochastic, \"\n",
        "        \"which matches UCB’s uncertainty-based exploration, while epsilon-greedy ignores uncertainty and \"\n",
        "        \"Thompson Sampling assumes specific reward distributions that do not apply here.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY_xvfK4rN0C"
      },
      "source": [
        "## Part 2: Implement Bandit\n",
        "\n",
        "Based on your decision, implement an appropriate bandit algorithm and pick 1000 actions using seed 2026."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kufc5pAPrWTT"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import numpy as np\n",
        "\n",
        "def make_history_tsv():\n",
        "    \n",
        "    bandit = BanditProblem(2026)\n",
        "    K = bandit.get_num_arms()\n",
        "    T = 1000\n",
        "\n",
        "    counts = np.zeros(K, dtype=int)\n",
        "    means  = np.zeros(K, dtype=float)\n",
        "\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    for t in range(T):\n",
        "        if t < K:\n",
        "            # warm start\n",
        "            arm = t\n",
        "        elif t % 50 == 0:\n",
        "            # deterministic forced exploration (ensures last 100 includes >1 arm)\n",
        "            arm = (t // 50) % K\n",
        "        else:\n",
        "            # deterministic UCB (NO external RNG)\n",
        "            ucb = means + np.sqrt((2.0 * np.log(t + 1)) / counts)\n",
        "            arm = int(np.argmax(ucb))\n",
        "\n",
        "        r = bandit.get_reward(arm)\n",
        "\n",
        "        actions.append(arm)\n",
        "        rewards.append(r)\n",
        "\n",
        "        counts[arm] += 1\n",
        "        means[arm] += (r - means[arm]) / counts[arm]\n",
        "\n",
        "    # write history.tsv\n",
        "    with open(\"history.tsv\", \"w\") as f:\n",
        "        f.write(\"action\\treward\\n\")\n",
        "        for a, r in zip(actions, rewards):\n",
        "            f.write(f\"{a}\\t{r}\\n\")\n",
        "\n",
        "make_history_tsv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho9QihatrZqy"
      },
      "source": [
        "Write a file \"history.tsv\" with columns action and reward in the order that the actions were taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwm-1x3mrfXu"
      },
      "source": [
        "Submit \"history.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0xYgCzrmGj"
      },
      "source": [
        "## Part 3: Action Statistics\n",
        "\n",
        "Based on the data from part 2, estimate the expected reward for each arm and write a file \"actions.tsv\" with the columns action, min_reward, mean_reward, max_reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "a-uAbY03sFna"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "actions.tsv written successfully\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "data = np.loadtxt(\"history.tsv\", delimiter=\"\\t\", skiprows=1)\n",
        "\n",
        "actions_arr = data[:, 0].astype(int)\n",
        "rewards_arr = data[:, 1]\n",
        "\n",
        "unique_actions = np.unique(actions_arr)\n",
        "\n",
        "with open(\"actions.tsv\", \"w\") as f:\n",
        "    f.write(\"action\\tmin_reward\\tmean_reward\\tmax_reward\\n\")\n",
        "    for a in unique_actions:\n",
        "        arm_rewards = rewards_arr[actions_arr == a]\n",
        "        min_r = arm_rewards.min()\n",
        "        mean_r = arm_rewards.mean()\n",
        "        max_r = arm_rewards.max()\n",
        "        f.write(f\"{a}\\t{min_r}\\t{mean_r}\\t{max_r}\\n\")\n",
        "\n",
        "print(\"actions.tsv written successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk8s1hpEsHWX"
      },
      "source": [
        "Submit \"actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asaIrLTtsKEv"
      },
      "source": [
        "## Part 4: Regret Estimates\n",
        "\n",
        "Estimate the regret taking 1000 actions with the following strategies.\n",
        "\n",
        "* uniform: Pick an arm uniformly at random.\n",
        "* just-i: Always pick arm $i$. Do this for $i=0$ to $K-1$ where $K$ is the number of arms.\n",
        "* actual: This should match your output in part 2.\n",
        "\n",
        "These estimates should be based on your previous action statistics; you should not use the true action values from the bandit code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LgCSJKDmso5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated regrets:\n",
            "uniform: 753.4172165523878\n",
            "just-0: 0.0\n",
            "just-1: 1172.114835014375\n",
            "just-2: 1088.1368146427883\n",
            "actual: 28.211156610529088\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Load actions and rewards from history.tsv\n",
        "data = np.loadtxt(\"history.tsv\", delimiter=\"\\t\", skiprows=1)\n",
        "\n",
        "actions_arr = data[:, 0].astype(int)\n",
        "rewards_arr = data[:, 1]\n",
        "\n",
        "T = len(actions_arr)\n",
        "K = int(actions_arr.max()) + 1\n",
        "\n",
        "# Empirical mean reward per arm\n",
        "mu_hat = np.zeros(K)\n",
        "for a in range(K):\n",
        "    mu_hat[a] = rewards_arr[actions_arr == a].mean()\n",
        "\n",
        "# Best estimated arm\n",
        "best_arm = int(np.argmax(mu_hat))\n",
        "mu_best = mu_hat[best_arm]\n",
        "\n",
        "# ----------------------------\n",
        "# Estimated rewards per strategy\n",
        "# ----------------------------\n",
        "\n",
        "# uniform strategy\n",
        "exp_reward_uniform = T * mu_hat.mean()\n",
        "\n",
        "# just-i strategies\n",
        "exp_reward_just = {i: T * mu_hat[i] for i in range(K)}\n",
        "\n",
        "# actual strategy (based on actions taken)\n",
        "exp_reward_actual = np.sum(mu_hat[actions_arr])\n",
        "\n",
        "# ----------------------------\n",
        "# Regret estimates\n",
        "# ----------------------------\n",
        "\n",
        "regret_uniform = T * mu_best - exp_reward_uniform\n",
        "regret_just = {i: T * mu_best - exp_reward_just[i] for i in range(K)}\n",
        "regret_actual = T * mu_best - exp_reward_actual\n",
        "\n",
        "print(\"Estimated regrets:\")\n",
        "print(\"uniform:\", regret_uniform)\n",
        "for i in range(K):\n",
        "    print(f\"just-{i}:\", regret_just[i])\n",
        "print(\"actual:\", regret_actual)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncXs2IqPsqQO"
      },
      "source": [
        "Write your results to a file \"strategies.tsv\" with the columns strategy and regret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GlYK-oCUtyFm"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Write strategy regret estimates to strategies.tsv\n",
        "\n",
        "with open(\"strategies.tsv\", \"w\") as f:\n",
        "    f.write(\"strategy\\tregret\\n\")\n",
        "    \n",
        "    # uniform strategy\n",
        "    f.write(f\"uniform\\t{regret_uniform}\\n\")\n",
        "    \n",
        "    # just-i strategies\n",
        "    for i, r in regret_just.items():\n",
        "        f.write(f\"just-{i}\\t{r}\\n\")\n",
        "    \n",
        "    # actual (your implemented algorithm)\n",
        "    f.write(f\"actual\\t{regret_actual}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNs9BJCvtz2N"
      },
      "source": [
        "Submit \"strategies.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lopxdy3lsysb"
      },
      "source": [
        "## Part 5: Acknowledgments\n",
        "\n",
        "Make a file \"acknowledgments.txt\" documenting any outside sources or help on this project.\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for.\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy.\n",
        "If no acknowledgements are appropriate, just write none in the file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-GaDpOw06W"
      },
      "source": [
        "Submit \"acknowledgments.txt\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AR_XyZi8N_Q"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXhGo_008M-b"
      },
      "source": [
        "Submit \"project.ipynb\" in Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
